# Auto-Codex Environment Variables
# Copy this file to .env and fill in your values

# =============================================================================
# AUTHENTICATION (REQUIRED)
# =============================================================================
# Auto-Codex uses the OpenAI Codex CLI.
#
# Codex CLI supports multiple auth methods (set one):
# - OPENAI_API_KEY (recommended for CLI)
# - CODEX_CODE_OAUTH_TOKEN (from `codex login --device-auth` or desktop profiles)
# - CODEX_CONFIG_DIR (path to Codex CLI config directory)
# - Or rely on the default ~/.codex config directory
# Install: npm install -g @openai/codex
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# CODEX_CODE_OAUTH_TOKEN=codex_oauth_xxxxxxxxxxxxxxxxxxxxx
# CODEX_CONFIG_DIR=/path/to/codex/config
# Disable default ~/.codex detection (OPTIONAL)
# AUTO_CODEX_DISABLE_DEFAULT_CODEX_CONFIG_DIR=1

# =============================================================================
# CODEX CLI SETTINGS (OPTIONAL)
# =============================================================================
# Codex CLI must be installed and available on PATH.
# It reads OPENAI_API_KEY from the environment when Auto-Codex runs it.
#
# Model override (OPTIONAL)
# Format: <model_name> or <model_name>-<reasoning_effort>
# Model names: gpt-5.2-codex, gpt-4o, etc.
# Reasoning effort: low, medium, high, xhigh
# Examples:
#   AUTO_BUILD_MODEL=gpt-5.2-codex          # Uses default reasoning effort (medium)
#   AUTO_BUILD_MODEL=gpt-5.2-codex-xhigh    # Uses xhigh reasoning effort
#   AUTO_BUILD_MODEL=gpt-4o                 # Uses gpt-4o with default reasoning
# Default: gpt-5.2-codex (with medium reasoning effort)
# AUTO_BUILD_MODEL=gpt-5.2-codex
#
# Reasoning effort override (OPTIONAL)
# Only used if model string doesn't include reasoning effort suffix
# Values: low, medium, high, xhigh
# AUTO_BUILD_REASONING_EFFORT=medium

# Keep Codex CLI sandbox enabled (OPTIONAL)
# Default is to keep sandboxing enabled; set to 1 to bypass (not recommended).
# AUTO_CODEX_BYPASS_CODEX_SANDBOX=0

# Security policy enforcement (OPTIONAL)
# By default, Auto-Codex sends security flags (allowed/blocked commands/paths) to Codex CLI.
# Set to "true" to disable this and use local-only pre-checks (backwards compatibility).
# AUTO_CODEX_LEGACY_SECURITY=true


# =============================================================================
# GIT/WORKTREE SETTINGS (OPTIONAL)
# =============================================================================
# Configure how Auto-Codex handles git worktrees for isolated builds.

# Default base branch for worktree creation (OPTIONAL)
# If not set, Auto-Codex will auto-detect main/master, or fall back to current branch.
# Common values: main, master, develop
# DEFAULT_BRANCH=main

# =============================================================================
# DEBUG MODE (OPTIONAL)
# =============================================================================
# Enable debug logging for development and troubleshooting.
# Shows detailed information about runner execution, agent calls, file operations.

# Enable debug mode (default: false)
# DEBUG=true

# Debug log level: 1=basic, 2=detailed, 3=verbose (default: 1)
# DEBUG_LEVEL=1

# Log to file instead of stdout (OPTIONAL)
# DEBUG_LOG_FILE=auto-codex/debug.log

# =============================================================================
# PRODUCTION SAFETY (OPTIONAL)
# =============================================================================
# Require healthcheck in release scripts (production-lite)
# AUTO_CODEX_PRODUCTION=true
# Enforce clean git status in health checks (strict)
# AUTO_CODEX_ENFORCE_CLEAN_GIT=true
# Enforce Codex sandbox in health checks (strict)
# AUTO_CODEX_ENFORCE_SANDBOX=true
# Enforce FalkorDB auth in health checks (strict)
# AUTO_CODEX_ENFORCE_FALKORDB_AUTH=true
# Enforce backup presence/age in health checks when Graphiti is enabled
# AUTO_CODEX_ENFORCE_BACKUPS=true
# Maximum age (days) for FalkorDB backups (0 = disable age check)
# AUTO_CODEX_BACKUP_MAX_AGE_DAYS=7

# Security audit log (optional)
# Writes allow/deny decisions for command execution to a file.
# AUTO_CODEX_AUDIT_LOG=/path/to/auto-codex/audit.log
# Audit log rotation (optional)
# Set to 0 to disable rotation.
# AUTO_CODEX_AUDIT_LOG_MAX_BYTES=5242880
# AUTO_CODEX_AUDIT_LOG_BACKUPS=5

# Limit task_logs.json growth (0 = unlimited)
# AUTO_CODEX_TASK_LOG_MAX_ENTRIES=2000

# =============================================================================
# LINEAR INTEGRATION (OPTIONAL)
# =============================================================================
# Enable Linear integration for real-time progress tracking in Linear.
# Get your API key from: https://linear.app/YOUR-TEAM/settings/api

# Linear API Key (OPTIONAL - enables Linear integration)
# LINEAR_API_KEY=lin_api_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Pre-configured Team ID (OPTIONAL - will auto-detect if not set)
# LINEAR_TEAM_ID=

# Pre-configured Project ID (OPTIONAL - will create project if not set)
# LINEAR_PROJECT_ID=

# =============================================================================
# UI SETTINGS (OPTIONAL)
# =============================================================================
# Enable fancy terminal UI with icons, colors, and interactive menus.
# Set to "false" to use plain text output (useful for CI/CD or log files).

# Enable fancy UI (default: true)
# ENABLE_FANCY_UI=true

# =============================================================================
# ELECTRON MCP SERVER (OPTIONAL)
# =============================================================================
# Enable Electron MCP server for AI agents to interact with and validate
# Electron desktop applications. This allows QA agents to capture screenshots,
# inspect windows, and validate Electron apps during the review process.
#
# The electron-mcp-server connects via Chrome DevTools Protocol to an Electron
# app running with remote debugging enabled.
#
# Prerequisites:
#   1. Start your Electron app with remote debugging:
#      ./YourElectronApp --remote-debugging-port=9222
#
#   2. For auto-codex-ui specifically (use the MCP-enabled scripts):
#      cd auto-codex-ui
#      pnpm run dev:mcp     # Development mode with MCP debugging
#      # OR for production build:
#      pnpm run start:mcp   # Production mode with MCP debugging
#
# Note: Only QA agents (qa_reviewer, qa_fixer) receive Electron MCP tools.
# Coder and Planner agents do NOT have access to these tools to minimize
# context token usage and keep agents focused on their roles.
#
# See: https://github.com/anthropics/anthropic-quickstarts/tree/main/mcp-electron-demo

# Enable Electron MCP integration (default: false)
# ELECTRON_MCP_ENABLED=true

# Chrome DevTools debugging port for Electron connection (default: 9222)
# ELECTRON_DEBUG_PORT=9222

# =============================================================================
# GRAPHITI MEMORY INTEGRATION V2 (OPTIONAL)
# =============================================================================
# Enable Graphiti-based persistent memory layer for cross-session context
# retention. Uses FalkorDB as the graph database backend.
#
# V2 supports multiple LLM and embedder providers:
#   - OpenAI (default)
#   - Anthropic (LLM only, use with Voyage for embeddings)
#   - Azure OpenAI
#   - Ollama (local, fully offline)
#
# Prerequisites:
#   1. Start FalkorDB: docker-compose up -d falkordb
#   2. Install Graphiti: pip install graphiti-core[falkordb]
#   3. Configure your chosen provider (see below)

# Enable Graphiti integration (default: false)
# Note: healthcheck validates Graphiti config from env/.env when enabled.
# GRAPHITI_ENABLED=true

# =============================================================================
# GRAPHITI: Provider Selection
# =============================================================================
# Choose which providers to use for LLM and embeddings.
# Default is "openai" for both.

# LLM provider: openai | anthropic | azure_openai | ollama | google
# GRAPHITI_LLM_PROVIDER=openai

# Embedder provider: openai | voyage | azure_openai | ollama | google
# GRAPHITI_EMBEDDER_PROVIDER=openai

# =============================================================================
# GRAPHITI: OpenAI Provider (Default)
# =============================================================================
# Use OpenAI for both LLM and embeddings. This is the simplest setup.
# Required: OPENAI_API_KEY

# OpenAI API Key
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# OpenAI Model for LLM (default: gpt-5-mini)
# OPENAI_MODEL=gpt-5-mini

# OpenAI Model for embeddings (default: text-embedding-3-small)
# Available: text-embedding-3-small (1536 dim), text-embedding-3-large (3072 dim)
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# =============================================================================
# GRAPHITI: Anthropic Provider (LLM only)
# =============================================================================
# Use Anthropic for LLM. Requires separate embedder (use Voyage or OpenAI).
# Example: GRAPHITI_LLM_PROVIDER=anthropic, GRAPHITI_EMBEDDER_PROVIDER=voyage
#
# Required: ANTHROPIC_API_KEY

# Anthropic API Key
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Anthropic Model (default: claude-sonnet-4-5-latest)
# GRAPHITI_ANTHROPIC_MODEL=claude-sonnet-4-5-latest

# =============================================================================
# GRAPHITI: Voyage AI Provider (Embeddings only)
# =============================================================================
# Use Voyage AI for embeddings. Commonly paired with Anthropic LLM.
# Get API key from: https://www.voyageai.com/
#
# Required: VOYAGE_API_KEY

# Voyage AI API Key
# VOYAGE_API_KEY=pa-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Voyage Embedding Model (default: voyage-3)
# Available: voyage-3 (1024 dim), voyage-3-lite (512 dim)
# VOYAGE_EMBEDDING_MODEL=voyage-3

# =============================================================================
# GRAPHITI: Google AI Provider
# =============================================================================
# Use Google AI (Gemini) for both LLM and embeddings.
# Get API key from: https://aistudio.google.com/apikey
#
# Required: GOOGLE_API_KEY

# Google AI API Key
# GOOGLE_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Google LLM Model (default: gemini-2.0-flash)
# GOOGLE_LLM_MODEL=gemini-2.0-flash

# Google Embedding Model (default: text-embedding-004)
# GOOGLE_EMBEDDING_MODEL=text-embedding-004

# =============================================================================
# GRAPHITI: Azure OpenAI Provider
# =============================================================================
# Use Azure OpenAI for both LLM and embeddings.
# Requires Azure OpenAI deployment with appropriate models.
#
# Required: AZURE_OPENAI_API_KEY, AZURE_OPENAI_BASE_URL

# Azure OpenAI API Key
# AZURE_OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Azure OpenAI Base URL (your Azure endpoint)
# AZURE_OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment

# Azure OpenAI Deployment Names
# AZURE_OPENAI_LLM_DEPLOYMENT=gpt-5
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small

# =============================================================================
# GRAPHITI: Ollama Provider (Local/Offline)
# =============================================================================
# Use Ollama for fully offline operation. No API keys required.
# Requires Ollama running locally with appropriate models pulled.
#
# Prerequisites:
#   1. Install Ollama: https://ollama.ai/
#   2. Pull models: ollama pull deepseek-r1:7b && ollama pull nomic-embed-text
#   3. Start Ollama server (usually auto-starts)
#
# Required: OLLAMA_LLM_MODEL, OLLAMA_EMBEDDING_MODEL, OLLAMA_EMBEDDING_DIM

# Ollama Server URL (default: http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434

# Ollama LLM Model
# Popular choices: deepseek-r1:7b, llama3.2:3b, mistral:7b, phi3:medium
# OLLAMA_LLM_MODEL=deepseek-r1:7b

# Ollama Embedding Model
# Popular choices: nomic-embed-text (768 dim), mxbai-embed-large (1024 dim)
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Ollama Embedding Dimension (REQUIRED for Ollama embeddings)
# Must match your embedding model's output dimension
# Common values: nomic-embed-text=768, mxbai-embed-large=1024, all-minilm=384
# OLLAMA_EMBEDDING_DIM=768

# =============================================================================
# GRAPHITI MCP SERVER (OPTIONAL - Agent-accessible knowledge graph)
# =============================================================================
# Enable Graphiti MCP server for direct agent access to knowledge graph.
# This is SEPARATE from GRAPHITI_ENABLED (Python library integration).
#
# With MCP server enabled, Codex agents can directly:
#   - Search for relevant context (search_nodes, search_facts)
#   - Add discoveries to the graph (add_episode)
#   - Retrieve recent sessions (get_episodes)
#
# Quick Start (uses docker-compose.yml in project root):
#   1. Set OPENAI_API_KEY in your .env file (or export it)
#   2. Run: docker-compose up -d
#   3. Set GRAPHITI_MCP_URL below
#
# Manual Docker (alternative):
#   docker run -d -p 8000:8000 \
#     -e DATABASE_TYPE=falkordb \
#     -e OPENAI_API_KEY=$OPENAI_API_KEY \
#     -e FALKORDB_URI=redis://host.docker.internal:6379 \
#     falkordb/graphiti-knowledge-graph-mcp
#
# See: https://docs.falkordb.com/agentic-memory/graphiti-mcp-server.html

# Graphiti MCP Server URL (setting this enables MCP integration)
# GRAPHITI_MCP_URL=http://localhost:8000/mcp/

# Graphiti MCP Server Port (for docker-compose, default: 8000)
# GRAPHITI_MCP_PORT=8000

# Optional: Override model settings for MCP server
# GRAPHITI_MODEL_NAME=gpt-4o-mini
# GRAPHITI_EMBEDDING_MODEL=text-embedding-3-small

# =============================================================================
# GRAPHITI: FalkorDB Connection Settings
# =============================================================================
# Configure the FalkorDB graph database connection.
# Start FalkorDB: docker run -p 6379:6379 falkordb/falkordb:latest

# FalkorDB Host (default: localhost)
# GRAPHITI_FALKORDB_HOST=localhost

# FalkorDB Port (default: 6379)
# GRAPHITI_FALKORDB_PORT=6379

# FalkorDB Password (default: empty)
# GRAPHITI_FALKORDB_PASSWORD=
# Optional alias used by docker-compose healthchecks and the desktop UI
# FALKORDB_PASSWORD=
# Production (recommended):
# FALKORDB_ARGS=--appendonly yes --requirepass <password>

# Graph Database Name (default: auto_claude_memory)
# Tip: For new installs, consider setting GRAPHITI_DATABASE=auto_codex_memory
# GRAPHITI_DATABASE=auto_claude_memory

# =============================================================================
# GRAPHITI: Miscellaneous Settings
# =============================================================================

# Disable Graphiti telemetry (default: true)
# GRAPHITI_TELEMETRY_ENABLED=false

# =============================================================================
# GRAPHITI: Example Configurations
# =============================================================================
#
# --- Example 1: OpenAI (simplest) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=openai
# GRAPHITI_EMBEDDER_PROVIDER=openai
# OPENAI_API_KEY=sk-xxxxxxxx
#
# --- Example 2: Anthropic + Voyage (high quality) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=anthropic
# GRAPHITI_EMBEDDER_PROVIDER=voyage
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxx
# VOYAGE_API_KEY=pa-xxxxxxxx
#
# --- Example 3: Ollama (fully offline) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=ollama
# GRAPHITI_EMBEDDER_PROVIDER=ollama
# OLLAMA_LLM_MODEL=deepseek-r1:7b
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# OLLAMA_EMBEDDING_DIM=768
#
# --- Example 4: Azure OpenAI (enterprise) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=azure_openai
# GRAPHITI_EMBEDDER_PROVIDER=azure_openai
# AZURE_OPENAI_API_KEY=xxxxxxxx
# AZURE_OPENAI_BASE_URL=https://your-resource.openai.azure.com/...
# AZURE_OPENAI_LLM_DEPLOYMENT=gpt-5
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small
#
# --- Example 5: Google AI (Gemini) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=google
# GRAPHITI_EMBEDDER_PROVIDER=google
# GOOGLE_API_KEY=AIzaSyxxxxxxxx
